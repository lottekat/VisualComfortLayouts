{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPCWFGbyj-f9"
      },
      "source": [
        "# Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hq0hIkm_j-gC"
      },
      "outputs": [],
      "source": [
        "#importing general packages and libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import ast\n",
        "import math\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4N577cmnj-gE"
      },
      "outputs": [],
      "source": [
        "#importing packages and libraries of ML framework\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Activation, Concatenate, Dropout, BatchNormalization, GlobalAveragePooling2D, LeakyReLU, Conv2D, Add, MaxPooling2D\n",
        "from tensorflow.keras import models, layers, regularizers\n",
        "from tensorflow.keras.models import Model, save_model, Sequential\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l2, l1_l2\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST19hoEWkB1r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "697c0eb4-6a5b-4745-e260-e955ccf46fca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JPiWJWgqo2L"
      },
      "source": [
        "# Function Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGUTZIhhpAOV"
      },
      "outputs": [],
      "source": [
        "def plot_training_validation_test_loss(history, test_loss, test_mae, file_path, name, title):\n",
        "    # Extract training mean absolute error (MAE) and training loss (MSE) from history and evaluation\n",
        "    training_mae = history.history['mae']\n",
        "    validation_mae = history.history['val_mae']\n",
        "    training_loss_mse = history.history['loss']\n",
        "    validation_loss_mse = history.history['val_loss']\n",
        "\n",
        "    # Number of epochs\n",
        "    epochs = range(1, len(training_loss_mse) + 1)\n",
        "    nr_epochs = len(epochs)\n",
        "\n",
        "    # Create a figure with two subplots\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    # Plot training loss with logarithmic y-axis\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs, training_mae, label='Training MAE', marker='o', color='#903C59', markersize=3)  # Loss color\n",
        "    plt.plot(epochs, validation_mae, label='Validation MAE', marker='o', color='#084C61', markersize=3)  # Loss color\n",
        "    plt.plot((nr_epochs), test_mae, 'r*', label='Test MAE', markersize=10, color='#F7D08A')  # Test Loss as a star\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.yscale('log')\n",
        "    plt.ylabel('MAE')\n",
        "    plt.title('Model mean absolute error')\n",
        "    plt.legend()\n",
        "    plt.xlim(0, (nr_epochs +1))\n",
        "\n",
        "    # Plot validation loss (mae) with logarithmic y-axis\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs, training_loss_mse, label='Training loss', marker='o', color='#903C59', markersize=3)  # MSE color\n",
        "    plt.plot(epochs, validation_loss_mse, label='Validation loss', marker='o', color='#084C61', markersize=3)  # MSE color\n",
        "    plt.plot((nr_epochs), test_loss, 'r*', label='Test loss', markersize=10, color='#F7D08A')  # Test MSE as a star\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.yscale('log')\n",
        "    plt.ylabel('MSE loss')\n",
        "    plt.title('Loss curve')\n",
        "    plt.legend()\n",
        "    plt.xlim(0, (nr_epochs +1))\n",
        "\n",
        "    # Adjust plot settings\n",
        "    plt.subplots_adjust(wspace=0.3)  # Adjust the width space between subplots\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "\n",
        "    # Save the figure with the specified filename and path, overwriting if it already exists\n",
        "    plt.savefig(os.path.join(file_path, f'{name}_losscurves'), bbox_inches='tight', dpi=300)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baATNXaqqI9Z"
      },
      "source": [
        "# Data preperation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAzDUGyP4B2s"
      },
      "outputs": [],
      "source": [
        "# Define paths of training data\n",
        "file_path_splitvariables_train1 = f'/content/drive/MyDrive/Thesis_LotteKat/ModelTraining/SplitVariables_train1.pkl'\n",
        "file_path_splitvariables_train2 = f'/content/drive/MyDrive/Thesis_LotteKat/ModelTraining/SplitVariables_train2.pkl'\n",
        "file_path_splitvariables_test = f'/content/drive/MyDrive/Thesis_LotteKat/ModelTraining/SplitVariables_test.pkl'\n",
        "file_path_splitvariables_val = f'/content/drive/MyDrive/Thesis_LotteKat/ModelTraining/SplitVariables_val.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MkOQ2oj4rL_"
      },
      "outputs": [],
      "source": [
        "# Load the variables from the file using pickle\n",
        "with open(file_path_splitvariables_train1, 'rb') as file:\n",
        "    loaded_variables1 = pickle.load(file)\n",
        "\n",
        "# Load the variables from the file using pickle\n",
        "with open(file_path_splitvariables_train2, 'rb') as file:\n",
        "    loaded_variables2 = pickle.load(file)\n",
        "\n",
        "# Combine the data from train1 and train2\n",
        "X_train_img = np.concatenate((loaded_variables1['X_train1_img'], loaded_variables2['X_train2_img']))\n",
        "X_train_feat = np.concatenate((loaded_variables1['X_train1_feat'], loaded_variables2['X_train2_feat']))\n",
        "y_train = np.concatenate((loaded_variables1['y_train1'], loaded_variables2['y_train2']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3VUNHZ26RB7"
      },
      "outputs": [],
      "source": [
        "# Load the variables from the file using pickle\n",
        "with open(file_path_splitvariables_val, 'rb') as file:\n",
        "    loaded_variables = pickle.load(file)\n",
        "\n",
        "# Extract the different variables in the file\n",
        "X_val_img = loaded_variables['X_val_img']\n",
        "X_val_feat = loaded_variables['X_val_feat']\n",
        "y_val = loaded_variables['y_val']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYBNmpBD6P1y"
      },
      "outputs": [],
      "source": [
        "# Load the variables from the file using pickle\n",
        "with open(file_path_splitvariables_test, 'rb') as file:\n",
        "    loaded_variables = pickle.load(file)\n",
        "\n",
        "# Extract the different variables in the file\n",
        "X_test_img = loaded_variables['X_test_img']\n",
        "X_test_feat = loaded_variables['X_test_feat']\n",
        "y_test = loaded_variables['y_test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLdkQ64K9Ee4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8372e2b-c394-4f23-d3fd-d1a4fee4a568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total =  55662\n",
            "number of images in sets (train, val, test) = 35623 8906 11133\n",
            "Ratio of (train, val, test) = 64:16:20\n"
          ]
        }
      ],
      "source": [
        "#Find the total length of each set\n",
        "train_len = len(X_train_img)\n",
        "val_len = len(X_val_img)\n",
        "test_len = len(X_test_img)\n",
        "\n",
        "#calculate the total length to check with folder\n",
        "total_set = train_len + val_len + test_len\n",
        "print('total = ', total_set)\n",
        "print('number of images in sets (train, val, test) =', train_len, val_len, test_len)\n",
        "\n",
        "#Calculate and print ratios of training, validation, test set respectively\n",
        "train_ratio, val_ratio, test_ratio = [len(dataset)/total_set * 100 for dataset in (X_train_img, X_val_img, X_test_img)]\n",
        "print(\"Ratio of (train, val, test) = {}:{}:{}\".format(round(train_ratio), round(val_ratio), round(test_ratio)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bas8zev3JaI"
      },
      "outputs": [],
      "source": [
        "# Split the labels into 2 parts, one for daylight and one for view\n",
        "y_train_daylight = y_train[:, :3]\n",
        "y_train_view = y_train[:, 3:]\n",
        "del y_train\n",
        "\n",
        "y_val_daylight = y_val[:, :3]\n",
        "y_val_view = y_val[:, 3:]\n",
        "del y_val\n",
        "\n",
        "y_test_daylight = y_test[:, :3]\n",
        "y_test_view = y_test[:, 3:]\n",
        "del y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CntQ6qLxgbzZ"
      },
      "source": [
        "# ML settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8n2llIH1dPK"
      },
      "outputs": [],
      "source": [
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5rlbxsm_t35"
      },
      "outputs": [],
      "source": [
        "# Define a learning rate annealing function with a minimum learning rate\n",
        "def lr_schedule(epoch):\n",
        "    iteration_per_epochs = 557\n",
        "    iteration = epoch * iteration_per_epochs\n",
        "\n",
        "    iteration_recude = 5000   # number of itertations when lr reduces\n",
        "    initial_lr = 0.001      # Initial learning rate\n",
        "    reduce_factor = 0.5    # Factor by which to reduce the learning rate\n",
        "    min_lr = 0.00000001      # Minimum learning rate\n",
        "\n",
        "    exponent = math.floor(iteration / iteration_recude)\n",
        "\n",
        "    new_learning_rate_temp = initial_lr * reduce_factor ** exponent\n",
        "    new_learning_rate = max(new_learning_rate_temp, min_lr)\n",
        "\n",
        "    return new_learning_rate\n",
        "\n",
        "# Create a learning rate scheduler callback\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb2LMmOEDVvQ"
      },
      "source": [
        "# Model 7 - hyper parameter adjustments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aODsDYjFdvs"
      },
      "source": [
        "## Model 7.5 - low l2 rate & low dropout rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHt6GfsMFhy_"
      },
      "outputs": [],
      "source": [
        "# Define save settings for trained model\n",
        "modelname = 'Model16_daylight'\n",
        "title_model ='model 16 - daylight model'\n",
        "folder_path_ResNets = f'/content/drive/MyDrive/Thesis_LotteKat/ModelTraining/{modelname}'\n",
        "checkpoint_filepath = f'{modelname}_checkpoint.h5'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the checkpoint callback\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_best_only=True,  # Save only the best model\n",
        "    monitor='val_loss',  # Monitor validation loss\n",
        "    mode='min',  # Mode can be 'min' or 'max' depending on what you want to monitor\n",
        "    verbose=1  # Print messages about model saving\n",
        ")"
      ],
      "metadata": {
        "id": "5NYwJUmlgZQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Architecure version ResNet 4 - daylight model\n",
        "# Define the input shape for images and numerical features\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "num_input = Input(shape=(2,))\n",
        "\n",
        "# Load the ResNet50 model\n",
        "base_model = ResNet50(weights=None, include_top=False, input_tensor=image_input)\n",
        "\n",
        "# Unfreeze the layers in the ResNet50 model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Find the output of the ResNet\n",
        "resnet_output = base_model.output\n",
        "\n",
        "# Add Batch Normalization for improved training stability\n",
        "batchnorm1 = BatchNormalization()(resnet_output)\n",
        "\n",
        "# Add a LeakyReLU activation function\n",
        "leaky1 = LeakyReLU(alpha=0.1)(batchnorm1)\n",
        "\n",
        "# Add the global average pooling layer and dense layers to the base model\n",
        "glob1 = GlobalAveragePooling2D()(leaky1)\n",
        "dropout1 = Dropout(0.3)(glob1)\n",
        "dense1 = Dense(256, activation='relu', kernel_regularizer=l2(0.0001))(dropout1)\n",
        "\n",
        "# Concatenate the output of the base model and the numerical input\n",
        "combined = Concatenate()([dense1, num_input])\n",
        "\n",
        "# Add Batch Normalization and LeakyReLU activation\n",
        "batchnorm2 = BatchNormalization()(combined)\n",
        "leaky2 = LeakyReLU(alpha=0.1)(batchnorm2)\n",
        "\n",
        "# Add another dense layer with dropout\n",
        "dense2 = Dense(128, activation='relu', kernel_regularizer=l2(0.0001))(leaky2)\n",
        "dropout2 = Dropout(0.3)(dense2)\n",
        "\n",
        "# Add the final dense layer for regression with 2 output units and linear activation\n",
        "output_layer = Dense(3, activation='linear')(dropout2)\n",
        "\n",
        "# Create the model and compile the model with mean squared error loss and mean absolute error metric\n",
        "model_v16d = Model(inputs=[image_input, num_input], outputs=output_layer)\n",
        "model_v16d.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])"
      ],
      "metadata": {
        "id": "u7Gbn-wxgccz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history_v16d = model_v16d.fit([X_train_img, X_train_feat], y_train_daylight,\n",
        "                          epochs=100, batch_size=64,\n",
        "                          validation_data=([X_val_img, X_val_feat], y_val_daylight),\n",
        "                          callbacks=[model_checkpoint_callback, early_stopping, lr_scheduler])"
      ],
      "metadata": {
        "id": "gOgM7O9ygyT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of model\n",
        "test_loss_v16d, test_mae_v16d = model_v16d.evaluate((X_test_img, X_test_feat), y_test_daylight, verbose=2)"
      ],
      "metadata": {
        "id": "0qGYQt1Mg1Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model to the correct folder\n",
        "# Check if the folder exists\n",
        "if not os.path.exists(folder_path_ResNets):\n",
        "    os.makedirs(folder_path_ResNets)\n",
        "\n",
        "# Save trained model\n",
        "model_v16d.save(os.path.join(folder_path_ResNets, f'{modelname}_model.h5'))\n",
        "\n",
        "# Check if the model file was saved\n",
        "if os.path.exists(os.path.join(folder_path_ResNets, f'{modelname}_model.h5')):\n",
        "    print('Model file saved successfully')\n",
        "else:\n",
        "    print('Model file not found')\n",
        "\n",
        "# Save history model for traceback\n",
        "with open(os.path.join(folder_path_ResNets, f'{modelname}_model_history.pkl'), 'wb') as file:\n",
        "    pickle.dump(history_v16d.history, file)  # Save the training history to a specific folder\n",
        "\n",
        "# Create a dictionary to store the test metrics\n",
        "test_metrics = {\n",
        "    'Test Loss': test_loss_v16d,\n",
        "    'Test MAE': test_mae_v16d\n",
        "}\n",
        "\n",
        "# Save the test metrics\n",
        "with open(os.path.join(folder_path_ResNets, f'{modelname}_model_test.txt'), 'w') as file:\n",
        "    for metric, value in test_metrics.items():\n",
        "        file.write(f'{metric}: {value}\\n')"
      ],
      "metadata": {
        "id": "3Q1H5cIXg5iD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training restuls, loss curve and MAE\n",
        "plot_training_validation_test_loss(history_v16d, test_loss_v16d, test_mae_v16d, folder_path_ResNets, name=modelname, title=title_model)"
      ],
      "metadata": {
        "id": "UnGkgZ9ug94Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save training metric points for easier access\n",
        "# Create a DataFrame from the training history\n",
        "history_v16d_data = {\n",
        "    'loss': history_v16d.history['loss'],\n",
        "    'val_loss': history_v16d.history['val_loss'],\n",
        "    'mae': history_v16d.history['mae'],\n",
        "    'val_mae': history_v16d.history['val_mae'],\n",
        "}\n",
        "df_history_v16d = pd.DataFrame(history_v16d_data)\n",
        "\n",
        "#Save training points\n",
        "path_training_pt = f'{folder_path_ResNets}/Model4_daylight_training_points.csv'\n",
        "df_history_v16d.to_csv(path_training_pt, index=False)"
      ],
      "metadata": {
        "id": "89Jqyzosg_uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model 7.5 - low l2 rate & low dropout rate - view"
      ],
      "metadata": {
        "id": "00cD4ZIzunVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define save settings for trained model\n",
        "modelname = 'Model16_view'\n",
        "title_model ='model 16 - view model'\n",
        "folder_path_ResNets = f'/content/drive/MyDrive/Thesis_LotteKat/ModelTraining/{modelname}'\n",
        "checkpoint_filepath = f'{modelname}_checkpoint.h5'"
      ],
      "metadata": {
        "id": "j2uBtcgmunul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the checkpoint callback\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_best_only=True,  # Save only the best model\n",
        "    monitor='val_loss',  # Monitor validation loss\n",
        "    mode='min',  # Mode can be 'min' or 'max' depending on what you want to monitor\n",
        "    verbose=1  # Print messages about model saving\n",
        ")"
      ],
      "metadata": {
        "id": "HWbyhiAAutuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Architecure version ResNet 4 - daylight model\n",
        "# Define the input shape for images and numerical features\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "num_input = Input(shape=(2,))\n",
        "\n",
        "# Load the ResNet50 model\n",
        "base_model = ResNet50(weights=None, include_top=False, input_tensor=image_input)\n",
        "\n",
        "# Unfreeze the layers in the ResNet50 model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Find the output of the ResNet\n",
        "resnet_output = base_model.output\n",
        "\n",
        "# Add Batch Normalization for improved training stability\n",
        "batchnorm1 = BatchNormalization()(resnet_output)\n",
        "\n",
        "# Add a LeakyReLU activation function\n",
        "leaky1 = LeakyReLU(alpha=0.1)(batchnorm1)\n",
        "\n",
        "# Add the global average pooling layer and dense layers to the base model\n",
        "glob1 = GlobalAveragePooling2D()(leaky1)\n",
        "dropout1 = Dropout(0.3)(glob1)\n",
        "dense1 = Dense(256, activation='relu', kernel_regularizer=l2(0.0001))(dropout1)\n",
        "\n",
        "# Concatenate the output of the base model and the numerical input\n",
        "combined = Concatenate()([dense1, num_input])\n",
        "\n",
        "# Add Batch Normalization and LeakyReLU activation\n",
        "batchnorm2 = BatchNormalization()(combined)\n",
        "leaky2 = LeakyReLU(alpha=0.1)(batchnorm2)\n",
        "\n",
        "# Add another dense layer with dropout\n",
        "dense2 = Dense(128, activation='relu', kernel_regularizer=l2(0.0001))(leaky2)\n",
        "dropout2 = Dropout(0.3)(dense2)\n",
        "\n",
        "# Add the final dense layer for regression with 2 output units and linear activation\n",
        "output_layer = Dense(2, activation='linear')(dropout2)\n",
        "\n",
        "# Create the model and compile the model with mean squared error loss and mean absolute error metric\n",
        "model_v16v = Model(inputs=[image_input, num_input], outputs=output_layer)\n",
        "model_v16v.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])"
      ],
      "metadata": {
        "id": "K3NVq-uluze-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history_v16v = model_v16v.fit([X_train_img, X_train_feat], y_train_view,\n",
        "                          epochs=100, batch_size=64,\n",
        "                          validation_data=([X_val_img, X_val_feat], y_val_view),\n",
        "                          callbacks=[model_checkpoint_callback, early_stopping, lr_scheduler])"
      ],
      "metadata": {
        "id": "pDFkpsw-u2IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of model\n",
        "test_loss_v16v, test_mae_v16v = model_v16v.evaluate((X_test_img, X_test_feat), y_test_view, verbose=2)"
      ],
      "metadata": {
        "id": "rwNWZnx3u8qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model to the correct folder\n",
        "# Check if the folder exists\n",
        "if not os.path.exists(folder_path_ResNets):\n",
        "    os.makedirs(folder_path_ResNets)\n",
        "\n",
        "# Save trained model\n",
        "model_v16v.save(os.path.join(folder_path_ResNets, f'{modelname}_model.h5'))\n",
        "\n",
        "# Check if the model file was saved\n",
        "if os.path.exists(os.path.join(folder_path_ResNets, f'{modelname}_model.h5')):\n",
        "    print('Model file saved successfully')\n",
        "else:\n",
        "    print('Model file not found')\n",
        "\n",
        "# Save history model for traceback\n",
        "with open(os.path.join(folder_path_ResNets, f'{modelname}_model_history.pkl'), 'wb') as file:\n",
        "    pickle.dump(history_v16v.history, file)  # Save the training history to a specific folder\n",
        "\n",
        "# Create a dictionary to store the test metrics\n",
        "test_metrics = {\n",
        "    'Test Loss': test_loss_v16v,\n",
        "    'Test MAE': test_mae_v16v\n",
        "}\n",
        "\n",
        "# Save the test metrics\n",
        "with open(os.path.join(folder_path_ResNets, f'{modelname}_model_test.txt'), 'w') as file:\n",
        "    for metric, value in test_metrics.items():\n",
        "        file.write(f'{metric}: {value}\\n')"
      ],
      "metadata": {
        "id": "QJaLfZ74imJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training restuls, loss curve and MAE\n",
        "plot_training_validation_test_loss(history_v16v, test_loss_v16v, test_mae_v16v, folder_path_ResNets, name=modelname, title=title_model)"
      ],
      "metadata": {
        "id": "iI7h9cFOjEET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save training metric points for easier access\n",
        "# Create a DataFrame from the training history\n",
        "history_v16v_data = {\n",
        "    'loss': history_v16v.history['loss'],\n",
        "    'val_loss': history_v16v.history['val_loss'],\n",
        "    'mae': history_v16v.history['mae'],\n",
        "    'val_mae': history_v16v.history['val_mae'],\n",
        "}\n",
        "df_history_v16v = pd.DataFrame(history_v16v_data)\n",
        "\n",
        "#Save training points\n",
        "path_training_pt = f'{folder_path_ResNets}/Model4_daylight_training_points.csv'\n",
        "df_history_v16v.to_csv(path_training_pt, index=False)"
      ],
      "metadata": {
        "id": "X2tYceN_jW6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.dtypes.base import StorageExtensionDtype\n",
        "StorageExtensionDtype"
      ],
      "metadata": {
        "id": "WNLPB4fBjZlb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0adeb1f2-7005-49de-ac11-9889e67fbf80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.dtypes.base.StorageExtensionDtype"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXVN9aNcNZ-F"
      },
      "outputs": [],
      "source": [
        "# Define save settings for trained model\n",
        "modelname = 'Model6_4_daylight'\n",
        "title_model ='model 6.4 - daylight model'\n",
        "folder_path_ResNets = f'/content/drive/MyDrive/Thesis_LotteKat/ModelTraining/{modelname}'\n",
        "checkpoint_filepath = f'{modelname}_checkpoint.h5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkBq1WmzNdbE"
      },
      "outputs": [],
      "source": [
        "# Define the checkpoint callback\n",
        "model_checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_best_only=True,  # Save only the best model\n",
        "    monitor='val_loss',  # Monitor validation loss\n",
        "    mode='min',  # Mode can be 'min' or 'max' depending on what you want to monitor\n",
        "    verbose=1  # Print messages about model saving\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KM2bUwVrNgKP"
      },
      "outputs": [],
      "source": [
        "# Architecure version ResNet 4 - daylight model\n",
        "# Define the input shape for images and numerical features\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "num_input = Input(shape=(2,))\n",
        "\n",
        "# Load the ResNet50 model\n",
        "base_model = ResNet50(weights=None, include_top=False, input_tensor=image_input)\n",
        "\n",
        "# Unfreeze the layers in the ResNet50 model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Find the output of the ResNet\n",
        "resnet_output = base_model.output\n",
        "\n",
        "# Add Batch Normalization for improved training stability\n",
        "batchnorm1 = BatchNormalization()(resnet_output)\n",
        "\n",
        "# Add a LeakyReLU activation function\n",
        "leaky1 = LeakyReLU(alpha=0.1)(batchnorm1)\n",
        "\n",
        "# Add the global average pooling layer and dense layers to the base model\n",
        "glob1 = GlobalAveragePooling2D()(leaky1)\n",
        "dropout1 = Dropout(0.3)(glob1)\n",
        "dense1 = Dense(256, activation='relu', kernel_regularizer=l2(0.01))(dropout1)\n",
        "\n",
        "# Concatenate the output of the base model and the numerical input\n",
        "combined = Concatenate()([dense1, num_input])\n",
        "\n",
        "# Add Batch Normalization and LeakyReLU activation\n",
        "batchnorm2 = BatchNormalization()(combined)\n",
        "leaky2 = LeakyReLU(alpha=0.1)(batchnorm2)\n",
        "\n",
        "# Add another dense layer with dropout\n",
        "dense2 = Dense(128, activation='relu', kernel_regularizer=l2(0.1))(leaky2)\n",
        "dropout2 = Dropout(0.6)(dense2)\n",
        "\n",
        "# Add the final dense layer for regression with 2 output units and linear activation\n",
        "output_layer = Dense(3, activation='linear')(dropout2)\n",
        "\n",
        "# Create the model and compile the model with mean squared error loss and mean absolute error metric\n",
        "model_v6Dd = Model(inputs=[image_input, num_input], outputs=output_layer)\n",
        "model_v6Dd.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSHEj2bqNlP8"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history_v6Dd = model_v6Dd.fit([X_train_img, X_train_feat], y_train_daylight,\n",
        "                          epochs=100, batch_size=64,\n",
        "                          validation_data=([X_val_img, X_val_feat], y_val_daylight),\n",
        "                          callbacks=[model_checkpoint_callback, early_stopping, lr_scheduler])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wntRyRVNp4J"
      },
      "outputs": [],
      "source": [
        "# Evaluation of model\n",
        "test_loss_v6Dd, test_mae_v6Dd = model_v6Dd.evaluate((X_test_img, X_test_feat), y_test_daylight, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdpNl1a4Ns8x"
      },
      "outputs": [],
      "source": [
        "# Save the trained model to the correct folder\n",
        "# Check if the folder exists\n",
        "if not os.path.exists(folder_path_ResNets):\n",
        "    os.makedirs(folder_path_ResNets)\n",
        "\n",
        "# Save trained model\n",
        "model_v6Dd.save(os.path.join(folder_path_ResNets, f'{modelname}_model.h5'))\n",
        "\n",
        "# Check if the model file was saved\n",
        "if os.path.exists(os.path.join(folder_path_ResNets, f'{modelname}_model.h5')):\n",
        "    print('Model file saved successfully')\n",
        "else:\n",
        "    print('Model file not found')\n",
        "\n",
        "# Save history model for traceback\n",
        "with open(os.path.join(folder_path_ResNets, f'{modelname}_model_history.pkl'), 'wb') as file:\n",
        "    pickle.dump(history_v6Dd.history, file)  # Save the training history to a specific folder\n",
        "\n",
        "# Create a dictionary to store the test metrics\n",
        "test_metrics = {\n",
        "    'Test Loss': test_loss_v6Dd,\n",
        "    'Test MAE': test_mae_v6Dd\n",
        "}\n",
        "\n",
        "# Save the test metrics\n",
        "with open(os.path.join(folder_path_ResNets, f'{modelname}_model_test.txt'), 'w') as file:\n",
        "    for metric, value in test_metrics.items():\n",
        "        file.write(f'{metric}: {value}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyeLZOcWNvEh"
      },
      "outputs": [],
      "source": [
        "# Plot the training restuls, loss curve and MAE\n",
        "plot_training_validation_test_loss(history_v6Dd, test_loss_v6Dd, test_mae_v6Dd, folder_path_ResNets, name=modelname, title=title_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jdQmlIGNybC"
      },
      "outputs": [],
      "source": [
        "# Save training metric points for easier access\n",
        "# Create a DataFrame from the training history\n",
        "history_v6Dd_data = {\n",
        "    'loss': history_v6Dd.history['loss'],\n",
        "    'val_loss': history_v6Dd.history['val_loss'],\n",
        "    'mae': history_v6Dd.history['mae'],\n",
        "    'val_mae': history_v6Dd.history['val_mae'],\n",
        "}\n",
        "df_history_v6Dd = pd.DataFrame(history_v6Dd_data)\n",
        "\n",
        "#Save training points\n",
        "path_training_pt = f'{folder_path_ResNets}/Model4_daylight_training_points.csv'\n",
        "df_history_v6Dd.to_csv(path_training_pt, index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}