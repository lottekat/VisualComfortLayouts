{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.cbook import boxplot_stats\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_stats(df, columns, names):\n",
    "  # Define lists for storing the boxplot statistics\n",
    "  q0s = []\n",
    "  q1s = []\n",
    "  meds = []\n",
    "  q3s = []\n",
    "  q4s = []\n",
    "\n",
    "  # Loop through each column and create a boxplot\n",
    "  for i, col in enumerate(columns):\n",
    "      # Get the boxplot statistics for the current column\n",
    "      data = df[col]\n",
    "      stats = boxplot_stats(data)\n",
    "\n",
    "      # Append the statistics to their respective lists\n",
    "      q0s.append(stats[0]['whislo'])\n",
    "      q1s.append(stats[0]['q1'])\n",
    "      meds.append(stats[0]['med'])\n",
    "      q3s.append(stats[0]['q3'])\n",
    "      q4s.append(stats[0]['whishi'])\n",
    "\n",
    "  # Create a new dataframe with the boxplot statistics\n",
    "  df_bp_stats = pd.DataFrame({\n",
    "      'category': names,\n",
    "      'lower_whisker': q0s,\n",
    "      'lower_quartile': q1s,\n",
    "      'median': meds,\n",
    "      'upper_quartile': q3s,\n",
    "      'upper_whisker': q4s\n",
    "  })\n",
    "\n",
    "  df_bp_stats.iloc[:, 1:] = df_bp_stats.iloc[:, 1:].applymap(lambda x: f\"{x:.0f}\" if isinstance(x, (int, float)) and x > 100 else (f\"{x:.1f}\" if isinstance(x, (int, float)) and x > 9 else f\"{x:.2f}\"))\n",
    "\n",
    "\n",
    "  return df_bp_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_box_plot_data(data, name):\n",
    "  # Get box plot statistics\n",
    "  stats = boxplot_stats(data)\n",
    "\n",
    "  # Extract relevant statistics\n",
    "  q0 = stats[0]['whislo']\n",
    "  q1 = stats[0]['q1']\n",
    "  median = stats[0]['med']\n",
    "  q2 = stats[0]['q3']\n",
    "  q4 = stats[0]['whishi']\n",
    "\n",
    "  IQR = q3-q1\n",
    "  xmin, xmax = np.percentile(data, [0.01, 99.99])\n",
    "\n",
    "  # Create a DataFrame with the statistics\n",
    "  df_stats = pd.DataFrame({#'view': name,\n",
    "                          'lower_whisker': q0,\n",
    "                          'lower_quartile': q1,\n",
    "                          'median': median,\n",
    "                          'upper_quartile': q2,\n",
    "                          'upper_whisker': q4}, index=[0])\n",
    "\n",
    "  # Print the DataFrame\n",
    "  print(df_stats)\n",
    "  return q0, q1, median, q3, q4, IQR, xmin, xmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Def performance levels and labels finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daylight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_daylight_level(daylight_median, room_type):\n",
    "    if daylight_median >= 750:\n",
    "        daylight_level = 'high'\n",
    "    \n",
    "    elif daylight_median >= 500:\n",
    "        daylight_level = 'med'\n",
    "    \n",
    "    elif daylight_median >= 300:\n",
    "        daylight_level = 'low'\n",
    "    \n",
    "    elif daylight_median < 300:\n",
    "        if room_type in ('ROOM', 'BEDROOM'):\n",
    "            if daylight_median >= 100:\n",
    "                daylight_level = 'min'\n",
    "            else:\n",
    "                daylight_level = 'insufficient'\n",
    "        elif room_type in ('LIVING_ROOM', 'DINING'):\n",
    "            if daylight_median >= 150:\n",
    "                daylight_level = 'min'\n",
    "            else:\n",
    "                daylight_level = 'insufficient'\n",
    "        elif room_type == 'KITCHEN':\n",
    "            if daylight_median >= 100:\n",
    "                daylight_level = 'min'\n",
    "            else:\n",
    "                daylight_level = 'insufficient'\n",
    "    \n",
    "    return daylight_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_daylight_label(group):\n",
    "    daylight_level_median = group['daylight_level_num'].median()\n",
    "    daylight_level_min = group['daylight_level_num'].min()\n",
    "\n",
    "    if daylight_level_min == 0:\n",
    "        daylight_label = 'F'\n",
    "    elif daylight_level_min == 4:\n",
    "        daylight_label = 'A'  \n",
    "    elif daylight_level_min == 1:\n",
    "        if daylight_level_median >= 2:\n",
    "            daylight_label = 'D'\n",
    "        else: \n",
    "            daylight_label = 'E'\n",
    "    elif daylight_level_min == 2:\n",
    "        if daylight_level_median >= 3:\n",
    "            daylight_label = 'C'\n",
    "        elif daylight_level_median >= 2:\n",
    "            daylight_label = 'D'\n",
    "        else: \n",
    "            daylight_label = 'E'\n",
    "    elif daylight_level_min == 3:\n",
    "        if daylight_level_median >= 4:\n",
    "            daylight_label = 'B'\n",
    "        elif daylight_level_median >= 3:\n",
    "            daylight_label = 'C'\n",
    "        elif daylight_level_median >= 2:\n",
    "            daylight_label = 'D'\n",
    "        else: \n",
    "            daylight_label = 'E'\n",
    "    else:\n",
    "        daylight_label = None\n",
    "    #print(f'min={daylight_level_min} median={daylight_level_median} label={daylight_label}')\n",
    "    return daylight_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping dictionary\n",
    "mapping_daylight_level = {\n",
    "    'insufficient': 0,\n",
    "    'min': 1,\n",
    "    'low': 2,\n",
    "    'med': 3,\n",
    "    'high': 4\n",
    "}\n",
    "\n",
    "# Define the mapping dictionary\n",
    "mapping_daylight_label = {\n",
    "    'A': 1,\n",
    "    'B': 0.8,\n",
    "    'C': 0.6,\n",
    "    'D': 0.4,\n",
    "    'E': 0.2,\n",
    "    'F': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_visible_views(row):\n",
    "    columns_to_check = ['view_ground_p80', 'view_landscape_p80', 'view_sky_p80']\n",
    "    count = sum(1 for col in columns_to_check if row[col] >= 0.477)\n",
    "    return count\n",
    "\n",
    "def find_view_level(landscape_visible, nr_layers):\n",
    "    if landscape_visible == 'no':\n",
    "        view_level = 'insufficient'\n",
    "    elif nr_layers == 1:\n",
    "        view_level = 'min'\n",
    "    elif nr_layers == 2:\n",
    "        view_level = 'med'\n",
    "    elif nr_layers == 3:\n",
    "        view_level = 'high'\n",
    "\n",
    "    return view_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_view_label(group):\n",
    "    view_level_median = group['view_level_num'].median()\n",
    "    view_level_min = group['view_level_num'].min()\n",
    "\n",
    "    if view_level_min == 0:\n",
    "        view_label = 'E'\n",
    "    elif view_level_min == 3:\n",
    "        view_label = 'A'  \n",
    "    elif view_level_min == 1:\n",
    "        if view_level_median >= 2:\n",
    "            view_label = 'C'\n",
    "        else: \n",
    "            view_label = 'D'\n",
    "    elif view_level_min == 2:\n",
    "        if view_level_median >= 3:\n",
    "            view_label = 'B'\n",
    "        elif view_level_median >= 2:\n",
    "            view_label = 'C'\n",
    "        else: \n",
    "            view_label = 'D'\n",
    "    #print(f'min={view_level_min} median={view_level_median} label={view_label}')\n",
    "    return view_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping dictionary\n",
    "mapping_view_level = {\n",
    "    'insufficient': 0,\n",
    "    'min': 1,\n",
    "    'med': 2,\n",
    "    'high': 3\n",
    "}\n",
    "\n",
    "mapping_view_label = {\n",
    "    'A': 1,\n",
    "    'B': 0.75,\n",
    "    'C': 0.5,\n",
    "    'D': 0.25,\n",
    "    'E': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_orientation_level(room_type, main_orientation):\n",
    "    if room_type == 'LIVING_ROOM':\n",
    "        if main_orientation == 'South-West':\n",
    "            orientation_level = 'high'\n",
    "        elif main_orientation in ('West', 'South'):\n",
    "            orientation_level = 'med'\n",
    "        elif main_orientation in ('North-West', 'South-East'):\n",
    "            orientation_level = 'low'\n",
    "        else: \n",
    "            orientation_level = 'min'\n",
    "\n",
    "    elif room_type == 'DINING':\n",
    "        if main_orientation in ('South-East', 'South'):\n",
    "            orientation_level = 'high'\n",
    "        elif main_orientation in ('East', 'South-West'):\n",
    "            orientation_level = 'med'\n",
    "        elif main_orientation in ('North-East', 'West'):\n",
    "            orientation_level = 'low'\n",
    "        else: \n",
    "            orientation_level = 'min'\n",
    "\n",
    "    elif room_type == 'KITCHEN':\n",
    "        if main_orientation == 'East':\n",
    "            orientation_level = 'high'\n",
    "        elif main_orientation in ('North-East', 'South-East'):\n",
    "            orientation_level = 'med'\n",
    "        elif main_orientation in ('North', 'South'):\n",
    "            orientation_level = 'low'\n",
    "        else: \n",
    "            orientation_level = 'min'\n",
    "\n",
    "    elif room_type in ('BEDROOM', 'ROOM'):\n",
    "        if main_orientation == 'South-East':\n",
    "            orientation_level = 'high'\n",
    "        elif main_orientation in ('East', 'South'):\n",
    "            orientation_level = 'med'\n",
    "        elif main_orientation in ('North-East', 'South-West'):\n",
    "            orientation_level = 'low'\n",
    "        else: \n",
    "            orientation_level = 'min'\n",
    "\n",
    "    elif room_type == 'OUTDOOR_SPACE':\n",
    "        if main_orientation == 'South':\n",
    "            orientation_level = 'high'\n",
    "        elif main_orientation in ('South-East','South-West'):\n",
    "            orientation_level = 'med'\n",
    "        elif main_orientation in ('North-East', 'East', 'West', 'North-West'):\n",
    "            orientation_level = 'low'\n",
    "        else: \n",
    "            orientation_level = 'min'\n",
    "\n",
    "    else: \n",
    "        orientation_level = 'nan'\n",
    "\n",
    "    return orientation_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_orientation_label(group):\n",
    "    orientation_level_median = group['orientation_level_num'].median()\n",
    "    orientation_level_min = group['orientation_level_num'].min()\n",
    "\n",
    "    if orientation_level_min == 3:\n",
    "        orientation_label = 'A'  \n",
    "    elif orientation_level_median >= 3:\n",
    "        orientation_label = 'B'\n",
    "    elif orientation_level_median >= 2:\n",
    "        orientation_label = 'C'\n",
    "    elif orientation_level_median >= 1:\n",
    "        orientation_label = 'D'\n",
    "    else:\n",
    "        orientation_label = 'E'\n",
    "    #print(f'min={orientation_level_min} median={orientation_level_median} label={orientation_label}')\n",
    "    return orientation_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping dictionary\n",
    "mapping_orientation_level = {\n",
    "    'min': 0,\n",
    "    'low': 1,\n",
    "    'med': 2,\n",
    "    'high': 3\n",
    "}\n",
    "\n",
    "mapping_orientation_label = mapping_daylight_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_overall_label_num(day_points, view_points, orientation_points):\n",
    "    penalty = 1\n",
    "\n",
    "    if day_points == 0 or view_points == 0 :\n",
    "        penalty = 0\n",
    "    \n",
    "    average = (day_points + view_points + orientation_points) / 3\n",
    "\n",
    "    overall_label = (average * penalty)\n",
    "\n",
    "    return overall_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_overall_label = {\n",
    "    'A': (1.0, 0.86),   #10\n",
    "    'B': (0.85, 0.71),  #15\n",
    "    'C': (0.70, 0.56),  #15\n",
    "    'D': (0.55, 0.36),  #20\n",
    "    'E': (0.35, 0.01),  #35\n",
    "    'F': (0, 0)\n",
    "}\n",
    "\n",
    "def find_overall_label(value):\n",
    "    for letter, (upper, lower) in mapping_overall_label.items():\n",
    "        if lower <= value <= upper:\n",
    "            return letter\n",
    "    return None  # Return None or another value to handle cases outside the defined ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine DF_info\n",
    "\n",
    "Merge all the seperate csv file togheter into one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine all the seperate df_info together into one\n",
    "\n",
    "# Set the directory where the info CSV files are located\n",
    "directory = 'C:/Users/Name/OneDrive - Delft University of Technology/Building Technology-Thesis/Swiss Dataset/SwissDataset_v3.0.0/InfoDataCSV_v2/'\n",
    "\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = glob.glob(os.path.join(directory, 'SwissDataset_v3.0.0_info_*.csv'))\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined info data\n",
    "df_combinedinfo = pd.DataFrame()\n",
    "\n",
    "# Iterate through each CSV file\n",
    "for file in csv_files:\n",
    "    # Read the CSV file into a DataFrame\n",
    "    data = pd.read_csv(file)\n",
    "    \n",
    "    # Append the data to the combined_data DataFrame\n",
    "    df_combinedinfo = df_combinedinfo.append(data, ignore_index=True)\n",
    "\n",
    "df_combinedinfo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove redudant room types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_combinedinfo['entity_subtype'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'orientation_distribution' in df_test\n",
    "df_combinedinfo['orientation_distribution'] = None\n",
    "df_combinedinfo['orientation_main'] = None\n",
    "\n",
    "# Create a dictionary to map orientations to indices\n",
    "orientation_mapping = {'North'          : 0,\n",
    "                        'North-East'    : 1,\n",
    "                        'East'          : 2,\n",
    "                        'South-East'    : 3,\n",
    "                        'South'         : 4,\n",
    "                        'South-West'    : 5,\n",
    "                        'West'          : 6,\n",
    "                        'North-West'    : 7 }\n",
    "\n",
    "# Create a mapping for 'OUTSIDE_DOOR' rooms\n",
    "outside_door_mapping = df_combinedinfo[df_combinedinfo['entity_subtype'] == 'OUTSIDE_DOOR'].set_index('apartment_id')['orientation']\n",
    "\n",
    "# Iterate over all rows in df_combinedinfo_rooms\n",
    "for index, row in df_combinedinfo.iterrows():\n",
    "    room_type = row['entity_subtype']\n",
    "    \n",
    "    if room_type in ('LIVING_ROOM', 'ROOM', 'KITCHEN', 'DINING', 'BEDROOM', 'STUDIO'):\n",
    "        window_count = row['nr_window_sides']\n",
    "        \n",
    "        if window_count == 0:\n",
    "            orientation_distribution = [0] * 8\n",
    "            main_direction = None\n",
    "            nr_of_directions = 0\n",
    "\n",
    "        if window_count > 0:\n",
    "            orientation_distribution = [0] * 8\n",
    "            orientation_str = row['orientation_percentage']\n",
    "            elements = re.findall(r\"\\['(.*?)' '(.*?)'\\]\", orientation_str)\n",
    "            orientation_list = [[element[0], element[1]] for element in elements]\n",
    "\n",
    "            max_percentage = 0\n",
    "            main_direction = None\n",
    "\n",
    "            for orientation, percentage in orientation_list:\n",
    "                orientation_index = orientation_mapping.get(orientation, -1)\n",
    "                if orientation_index != -1:\n",
    "                    orientation_distribution[orientation_index] = round(float(percentage) / 100, 2)\n",
    "                    count_of_zeros = orientation_distribution.count(0)\n",
    "                    nr_of_directions = 8 - count_of_zeros\n",
    "\n",
    "                current_percentage = float(percentage)\n",
    "                if current_percentage > max_percentage:\n",
    "                    max_percentage = current_percentage\n",
    "                    main_direction = orientation\n",
    "\n",
    "        else:\n",
    "            orientation_distribution = [0] * 8\n",
    "            nr_of_directions = 0\n",
    "\n",
    "        df_combinedinfo.at[index, 'orientation_distribution'] = str(orientation_distribution)\n",
    "        df_combinedinfo.at[index, 'orientation_main'] = str(main_direction)\n",
    "        df_combinedinfo.at[index, 'nr_window_sides'] = nr_of_directions\n",
    "    \n",
    "    elif room_type == 'OUTDOOR_SPACE':\n",
    "        app_id = row['apartment_id']\n",
    "        area_id = row['area_id']\n",
    "        if app_id in outside_door_mapping:\n",
    "            main_direction = outside_door_mapping[app_id]\n",
    "            \n",
    "            # Check if main_direction is a pandas Series\n",
    "            if isinstance(main_direction, pd.Series):\n",
    "                filtered_rows = df_combinedinfo[(df_combinedinfo['apartment_id'] == app_id) & (df_combinedinfo['entity_subtype'] == 'OUTSIDE_DOOR')]\n",
    "                filtered_rows_with_area_id = filtered_rows[(filtered_rows['door_connection1'] == area_id) | (filtered_rows['door_connection2'] == area_id)]\n",
    "\n",
    "                number_of_rows = len(filtered_rows_with_area_id)\n",
    "                \n",
    "                if number_of_rows == 1:\n",
    "                    main_direction = filtered_rows_with_area_id['orientation'].iloc[0]\n",
    "                    df_combinedinfo.at[index, 'orientation_main'] = main_direction\n",
    "\n",
    "                elif number_of_rows > 1:\n",
    "                    directions = set(filtered_rows_with_area_id['orientation'].values)\n",
    "                    number_of_directions = len(directions)\n",
    "\n",
    "                    if number_of_directions == 1:\n",
    "                        main_direction = directions.pop()\n",
    "                        df_combinedinfo.at[index, 'orientation_main'] = main_direction\n",
    "\n",
    "                    else:\n",
    "                        if not filtered_rows_with_area_id.empty:\n",
    "                            # Group the rows by orientation and calculate the total window area for each orientation\n",
    "                            orientation_window_area = filtered_rows_with_area_id.groupby('orientation')['window_area'].sum()\n",
    "\n",
    "                            # Find the orientation with the highest total window area\n",
    "                            main_direction = orientation_window_area.idxmax()\n",
    "                            df_combinedinfo.at[index, 'orientation_main'] = main_direction\n",
    "                \n",
    "            else:\n",
    "                df_combinedinfo.at[index, 'orientation_main'] = main_direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the outdoorspaces \n",
    "filtered_data = df_combinedinfo[df_combinedinfo['entity_subtype'] == 'OUTDOOR_SPACE']\n",
    "\n",
    "# Get the apartment_id values from the filtered data and store them in a list\n",
    "area_ids = filtered_data['area_id'].tolist()\n",
    "print(len(area_ids))\n",
    "\n",
    "# Exclude the rows with the filtered area_ids from the original DataFrame\n",
    "df_combinedinfo_outdoorspace = df_combinedinfo[df_combinedinfo['area_id'].isin(area_ids)]\n",
    "df_combinedinfo_outdoorspace = df_combinedinfo_outdoorspace[df_combinedinfo_outdoorspace['entity_subtype'] == 'OUTDOOR_SPACE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'entity_subtype' is not in 'redundant_entities'\n",
    "redundant_entities = ['OUTDOOR_SPACE', 'OTHER', 'VOID', 'BATHROOM', 'CORRIDOR', 'STOREROOM', 'CIRCULATION']\n",
    "filtered_data = df_combinedinfo[df_combinedinfo['entity_subtype'].isin(redundant_entities)]\n",
    "\n",
    "# Get the apartment_id values from the filtered data and store them in a list\n",
    "area_ids = filtered_data['area_id'].tolist()\n",
    "print(len(area_ids))\n",
    "\n",
    "# Exclude the rows with the filtered area_ids from the original DataFrame\n",
    "df_combinedinfo1 = df_combinedinfo[~df_combinedinfo['area_id'].isin(area_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_combinedinfo1['entity_subtype'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make feature orientation distribution usable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store doors seperately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean DF_geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create geometry dataframe\n",
    "#Call CSV file of dataset, and import dataset using Pandas\n",
    "path_geom = f'C:/Users/Name/OneDrive - Delft University of Technology/Building Technology-Thesis/Swiss Dataset/SwissDataset_v3.0.0/SwissDataset_v3.0.0_geometries.csv'\n",
    "Swiss_geom = pd.read_csv(path_geom)\n",
    "\n",
    "#Create (pandas) dataframe\n",
    "df_geom = pd.DataFrame(Swiss_geom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 0: duplicate rows and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicates\n",
    "df_clean0A_geom = df_geom.drop_duplicates()\n",
    "\n",
    "#Filter out features\n",
    "df_clean0B_geom = df_clean0A_geom[df_clean0A_geom[\"entity_type\"] != \"feature\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 1: multi story appartments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove the apartments with multiple floors -> outside scope of thesis\n",
    "# Count the number of unique unit_ids per apartment_id\n",
    "apartment_unit_counts = df_clean0B_geom.groupby('apartment_id')['unit_id'].nunique()\n",
    "\n",
    "# Get the apartment_ids with multiple unique unit_ids\n",
    "apartments_with_multiple_floors = apartment_unit_counts[apartment_unit_counts > 1].index\n",
    "print(len(set(apartments_with_multiple_floors)))\n",
    "\n",
    "# Remove rows with apartment_ids that have multiple unique unit_ids\n",
    "df_clean1_geom = df_clean0B_geom[~df_clean0B_geom['apartment_id'].isin(apartments_with_multiple_floors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the number of apartments with more than one story in the dataframe\n",
    "num_app_mult = df_geom['apartment_id'].nunique() - df_clean1_geom['apartment_id'].nunique()\n",
    "num_room_mult = df_geom['area_id'].nunique() - df_clean1_geom['area_id'].nunique()\n",
    "print(f'Multiple story apartments: Apartments: {num_app_mult}, Rooms: {num_room_mult}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combinedinfo1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combinedinfo1.drop_duplicates()\n",
    "\n",
    "#Store the door info temporarily in a different frame and bring back at the end\n",
    "df_info_doors = df_combinedinfo1[(df_combinedinfo1['entity_subtype'] == 'DOOR')]\n",
    "df_combinedinfo2 = df_combinedinfo1[(df_combinedinfo1['entity_subtype'] != 'DOOR')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean DF_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create simulation dataframe\n",
    "#Call CSV file of dataset, and import dataset using Pandas\n",
    "path_sim = f'C:/Users/Name/OneDrive - Delft University of Technology/Building Technology-Thesis/Swiss Dataset/SwissDataset_v3.0.0/SwissDataset_v3.0.0_simulations.csv'\n",
    "Swiss_sim = pd.read_csv(path_sim)\n",
    "\n",
    "#Create (pandas) dataframe\n",
    "df_sim = pd.DataFrame(Swiss_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sim.drop_duplicates()\n",
    "\n",
    "# Filter rows where 'entity_subtype' is not in 'redundant_entities'\n",
    "redundant_entities = ['Balcony', 'SunRoom', 'Loggia', 'Bathroom', 'Corridor', 'StorageRoom']\n",
    "filtered_data = df_sim[df_sim['layout_area_type'].isin(redundant_entities)]\n",
    "\n",
    "# Get the apartment_id values from the filtered data and store them in a list\n",
    "area_ids = filtered_data['area_id'].tolist()\n",
    "print(len(area_ids))\n",
    "\n",
    "# Exclude the rows with the filtered area_ids from the original DataFrame\n",
    "df_sim1 = df_sim[~df_sim['area_id'].isin(area_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting dataframes:\n",
    "df_combinedinfo2        # dataframe all info from feature creation\n",
    "df_clean1_geom          # dataframe all geometries\n",
    "df_sim1                 # dataframe all simulation data\n",
    "\n",
    "print('continue with these dataframes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 2: apartments outside simulation dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the apartment_id values from the filtered data and store them in sets\n",
    "apartment_ids_info2_set = set(df_combinedinfo2['apartment_id'])\n",
    "apartment_ids_sim1_set = set(df_sim1['apartment_id'])\n",
    "\n",
    "# Find the common apartment_ids using set intersection\n",
    "common_apartment_ids = apartment_ids_info2_set.intersection(apartment_ids_sim1_set)\n",
    "\n",
    "# Exclude the rows with the filtered apartment_ids from the original DataFrame\n",
    "df_clean2_sim = df_sim1[df_sim1['apartment_id'].isin(common_apartment_ids)]\n",
    "df_clean2_info = df_combinedinfo2[df_combinedinfo2['apartment_id'].isin(common_apartment_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the number of apartments without simulation data in the dataframe\n",
    "num_app_sim = df_combinedinfo2['apartment_id'].nunique() - df_clean2_info['apartment_id'].nunique()\n",
    "num_room_sim = df_combinedinfo2['area_id'].nunique() - df_clean2_info['area_id'].nunique()\n",
    "print(f'Apartments outside simulation dataset: Apartments: {num_app_sim}, Rooms: {num_room_sim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 3: shared windows geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all the apartments with shared windows (1)\n",
    "# Filter out rows with area_id equal to 1 or 3\n",
    "filtered_data = df_clean2_info[(df_clean2_info['area_id'] == 1)]\n",
    "\n",
    "# Get the apartment_id values from the filtered data and store them in a list\n",
    "apartment_ids = filtered_data['apartment_id'].tolist()\n",
    "\n",
    "# Exclude the rows with the filtered apartment_ids from the original DataFrame\n",
    "df_clean3_info = df_clean2_info[~df_clean2_info['apartment_id'].isin(apartment_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the number of apartments with shared windows\n",
    "num_app_shared = df_clean2_info['apartment_id'].nunique() - df_clean3_info['apartment_id'].nunique()\n",
    "num_room_shared = df_clean2_info['area_id'].nunique() - df_clean3_info['area_id'].nunique()\n",
    "print(f'Apartments shared windows: Apartments: {num_app_shared}, Rooms: {num_room_shared}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean3_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 4: other window geometry problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of all the apartments with other wrong apartment geomerties (3)\n",
    "# Filter out rows with area_id equal to 1 or 3\n",
    "filtered_data = df_clean3_info[(df_clean3_info['area_id'] == 3)]\n",
    "\n",
    "# Get the apartment_id values from the filtered data and store them in a list\n",
    "apartment_ids = filtered_data['apartment_id'].tolist()\n",
    "\n",
    "# Exclude the rows with the filtered apartment_ids from the original DataFrame\n",
    "df_clean4_info = df_clean3_info[~df_clean3_info['apartment_id'].isin(apartment_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update the geom and simulation dataframes, needed for the next cleaning steps\n",
    "# Get the apartment_id values from the filtered data and store them in a list\n",
    "apartment_ids = df_clean4_info['apartment_id'].tolist()\n",
    "\n",
    "# Exclude the rows with the filtered apartment_ids from the original DataFrame\n",
    "df_clean4_geom = df_clean1_geom[df_clean1_geom['apartment_id'].isin(apartment_ids)]\n",
    "df_clean4_sim = df_clean2_sim[df_clean2_sim['apartment_id'].isin(apartment_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe with only rooms\n",
    "window_types = ['WINDOW', 'OUTSIDE_DOOR']\n",
    "df_clean4_info_room = df_clean4_info[~df_clean4_info['entity_subtype'].isin(window_types)]\n",
    "\n",
    "# Convert the \"area_id\" column to integers\n",
    "df_clean4_info_room['area_id'] = df_clean4_info_room['area_id'].astype(float)\n",
    "df_clean4_info_room['area_id'] = df_clean4_info_room['area_id'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the number of other difficulties in the apartments\n",
    "num_app_otherprob = df_clean3_info['apartment_id'].nunique() - df_clean4_info['apartment_id'].nunique()\n",
    "num_room_otherprob = df_clean3_info['area_id'].nunique() - df_clean4_info['area_id'].nunique()\n",
    "print(f'Apartments other window difficulties: {num_app_otherprob}, Rooms: {num_room_otherprob}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean4_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 5: Rooms without daylight values but with sky values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create needed view columns by combining data\n",
    "# Select only desired columns for view dataframe\n",
    "df_views = df_clean4_sim.loc[:, ['area_id', 'apartment_id', 'layout_area_type']]\n",
    "\n",
    "#Create columns with the four view categories for mean\n",
    "df_views['view_site_mean'] = df_clean4_sim['view_site_mean']\n",
    "df_views['view_ground_mean'] = df_clean4_sim[['view_ground_mean', 'view_highways_mean', 'view_railway_tracks_mean','view_tertiary_streets_mean', 'view_secondary_streets_mean', 'view_primary_streets_mean', 'view_pedestrians_mean']].sum(axis=1)\n",
    "df_views['view_landscape_nature_mean'] = df_clean4_sim[['view_greenery_mean', 'view_water_mean', 'view_mountains_class_2_mean', 'view_mountains_class_3_mean', 'view_mountains_class_4_mean', 'view_mountains_class_5_mean', 'view_mountains_class_6_mean']].sum(axis=1)\n",
    "df_views['view_landscape_urban_mean'] = df_clean4_sim['view_buildings_mean']\n",
    "df_views['view_sky_mean'] = df_clean4_sim['view_sky_mean']\n",
    "\n",
    "#Make percantage\n",
    "df_views[['view_site_mean', 'view_ground_mean', 'view_landscape_nature_mean', 'view_landscape_urban_mean', 'view_sky_mean']] *= 100\n",
    "\n",
    "#Find the sum off all the view categories to check that is counts up to 100%\n",
    "df_views['view_total_mean'] = df_views[['view_site_mean', 'view_ground_mean', 'view_landscape_nature_mean', 'view_landscape_urban_mean', 'view_sky_mean']].sum(axis=1)\n",
    "\n",
    "#Create columns with the four view categories for median\n",
    "df_views['view_site_median'] = df_clean4_sim['view_site_median']\n",
    "df_views['view_ground_median'] = df_clean4_sim[['view_ground_median', 'view_highways_median', 'view_railway_tracks_median','view_tertiary_streets_median', 'view_secondary_streets_median', 'view_primary_streets_median', 'view_pedestrians_median']].sum(axis=1)\n",
    "df_views['view_landscape_nature_median'] = df_clean4_sim[['view_greenery_median', 'view_water_median', 'view_mountains_class_2_median', 'view_mountains_class_3_median', 'view_mountains_class_4_median', 'view_mountains_class_5_median', 'view_mountains_class_6_median']].sum(axis=1)\n",
    "df_views['view_landscape_urban_median'] = df_clean4_sim['view_buildings_median']\n",
    "df_views['view_sky_median'] = df_clean4_sim['view_sky_median']\n",
    "\n",
    "#Make percantage\n",
    "df_views[['view_site_median', 'view_ground_median', 'view_landscape_nature_median', 'view_landscape_urban_median', 'view_sky_median']] *= 100\n",
    "\n",
    "\n",
    "#Create columns with the four view categories for p80\n",
    "df_views['view_site_p80'] = df_clean4_sim['view_site_p80']\n",
    "df_views['view_ground_p80'] = df_clean4_sim[['view_ground_p80', 'view_highways_p80', 'view_railway_tracks_p80','view_tertiary_streets_p80', 'view_secondary_streets_p80', 'view_primary_streets_p80', 'view_pedestrians_p80']].sum(axis=1)\n",
    "df_views['view_landscape_nature_p80'] = df_clean4_sim[['view_greenery_p80', 'view_water_p80', 'view_mountains_class_2_p80', 'view_mountains_class_3_p80', 'view_mountains_class_4_p80', 'view_mountains_class_5_p80', 'view_mountains_class_6_p80']].sum(axis=1)\n",
    "df_views['view_landscape_urban_p80'] = df_clean4_sim['view_buildings_p80']\n",
    "df_views['view_sky_p80'] = df_clean4_sim['view_sky_p80']\n",
    "\n",
    "#Make percantage\n",
    "df_views[['view_site_p80', 'view_ground_p80', 'view_landscape_nature_p80', 'view_landscape_urban_p80', 'view_sky_p80']] *= 100\n",
    "\n",
    "#Find the sum off all the view categories to check that is counts up to 100%\n",
    "df_views['view_total_p80'] = df_views[['view_site_p80', 'view_ground_p80', 'view_landscape_nature_p80', 'view_landscape_urban_p80', 'view_sky_p80']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create needed daylight columns and change to lux\n",
    "# Multiply all the daylight by 1000 to convert from kilolux to lux\n",
    "df_sunlight = df_clean4_sim.loc[:, ['area_id', 'apartment_id', 'layout_area_type', 'sun_201803211200_median', 'sun_201806211200_median', 'sun_201812211200_median', 'sun_201803211200_mean', 'sun_201806211200_mean', 'sun_201812211200_mean']]\n",
    "\n",
    "df_sunlight[['daylight_21Mar1200_median', 'daylight_21Jun1200_median', 'daylight_21Dec1200_median', 'daylight_21Mar1200_mean', 'daylight_21Jun1200_mean', 'daylight_21Dec1200_mean']] = df_sunlight[['sun_201803211200_median', 'sun_201806211200_median', 'sun_201812211200_median', 'sun_201803211200_mean', 'sun_201806211200_mean', 'sun_201812211200_mean']].multiply(1000).round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all simulation data to info dataframe\n",
    "# Merge df info with the view simulation results\n",
    "df_clean4_info_room = pd.merge(df_clean4_info_room, df_views[['apartment_id', 'area_id', 'view_site_mean', 'view_ground_mean', 'view_landscape_nature_mean', 'view_landscape_urban_mean', 'view_sky_mean','view_total_mean', 'view_site_median', 'view_ground_median', 'view_landscape_nature_median', 'view_landscape_urban_median', 'view_sky_median', 'view_site_p80', 'view_ground_p80', 'view_landscape_nature_p80',\t'view_landscape_urban_p80',\t'view_sky_p80']], on=['apartment_id', 'area_id'], how='left')\n",
    "\n",
    "# Merge df info with the daylight simulation results\n",
    "df_clean4_info_room = pd.merge(df_clean4_info_room, df_sunlight[['apartment_id', 'area_id', 'daylight_21Mar1200_median', 'daylight_21Jun1200_median', 'daylight_21Dec1200_median', 'daylight_21Mar1200_mean', 'daylight_21Jun1200_mean', 'daylight_21Dec1200_mean']], on=['apartment_id', 'area_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rooms that have no daylight value but a high sky view value \n",
    "filtered_data = df_clean4_info_room[(df_clean4_info_room['daylight_21Mar1200_median'] == 0) & (df_clean4_info_room['view_sky_mean'] > 0.477)]\n",
    "\n",
    "# Get the apartment_id values from the filtered data and store them in a list\n",
    "apartment_ids = filtered_data['apartment_id'].tolist()\n",
    "\n",
    "# Exclude the rows with the filtered apartment_ids from the original DataFrame\n",
    "df_clean5_info_room = df_clean4_info_room[~df_clean4_info_room['apartment_id'].isin(apartment_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rooms without daylight but with view to sky\n",
    "num_app_daysky = df_clean4_info_room['apartment_id'].nunique() - df_clean5_info_room['apartment_id'].nunique()\n",
    "num_room_daysky = df_clean4_info_room['area_id'].nunique() - df_clean5_info_room['area_id'].nunique()\n",
    "print(f'Rooms without daylight with sky view: {num_app_daysky}, Rooms: {num_room_daysky}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 6: simulation na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values in the 'sun_201803211200_mean' and 'view_sky' columns\n",
    "# Store the app_id values of rows to be dropped\n",
    "dropped_app_ids_labels = df_clean5_info_room[df_clean5_info_room['daylight_21Mar1200_median'].isna() | df_clean5_info_room['daylight_21Jun1200_median'].isna() |df_clean5_info_room['daylight_21Dec1200_median'].isna() | df_clean5_info_room['view_ground_p80'].isna() | df_clean5_info_room['view_sky_p80'].isna()]['apartment_id'].tolist()\n",
    "dropped_app_ids_features = df_clean5_info_room[df_clean5_info_room['layout_area'].isna() | df_clean5_info_room['room_depth_ratio'].isna() |df_clean5_info_room['nr_window_sides'].isna() | df_clean5_info_room['view_site_p80'].isna() | df_clean5_info_room['view_landscape_nature_p80'].isna() | df_clean5_info_room['view_landscape_urban_p80'].isna() | df_clean5_info_room['elevation'].isna() | df_clean5_info_room['window_floor_ratio'].isna()]['apartment_id'].tolist()\n",
    "dropped_app_ids = list(set(dropped_app_ids_labels + dropped_app_ids_features))\n",
    "\n",
    "# Exclude the rows with the filtered apartment_ids from the original DataFrame\n",
    "df_clean6_info_room = df_clean5_info_room[~df_clean5_info_room['apartment_id'].isin(dropped_app_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the number of rooms without simulation values\n",
    "num_app_nan = df_clean5_info_room['apartment_id'].nunique() - df_clean6_info_room['apartment_id'].nunique()\n",
    "num_room_nan = df_clean5_info_room['area_id'].nunique() - df_clean6_info_room['area_id'].nunique()\n",
    "print(f'Rooms without daylight with sky view: {num_app_nan}, Rooms: {num_room_nan}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean 7: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove outliers for sky view p80 and daylight median and filter out all appartments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_room_complete4 is your DataFrame\n",
    "data_sky = df_clean6_info_room['view_sky_p80']\n",
    "\n",
    "# Calculate the mean and standard deviation for the 'sky' column\n",
    "mean_sky = data_sky.mean()\n",
    "std_sky = data_sky.std()\n",
    "\n",
    "# Create a Boolean mask for outliers in the 'sky_21Mar1200_median' column a threshold for outliers (e.g., 3 times the standard deviation)\n",
    "outlier_mask_sky = (data_sky - mean_sky).abs() > 3 * std_sky\n",
    "\n",
    "# Filter the DataFrame to keep only rows without outliers in the 'sky_21Mar1200_median' column\n",
    "sky_outlier_rooms = df_clean6_info_room[outlier_mask_sky]\n",
    "\n",
    "# Get the apartment_id values from the filtered data and store them in a list\n",
    "apartment_ids = sky_outlier_rooms['apartment_id'].tolist()\n",
    "print(len(set(apartment_ids)))\n",
    "\n",
    "# Exclude the rows with the filtered apartment_ids from the original DataFrame\n",
    "df_clean7_info_room_sky = df_clean6_info_room[~df_clean6_info_room['apartment_id'].isin(apartment_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_room_complete4 is your DataFrame\n",
    "data_ground = df_clean6_info_room['view_ground_p80']\n",
    "\n",
    "# Calculate the mean and standard deviation for the 'ground' column\n",
    "mean_ground = data_ground.mean()\n",
    "std_ground = data_ground.std()\n",
    "\n",
    "# Create a Boolean mask for outliers in the 'ground_21Mar1200_median' column a threshold for outliers (e.g., 3 times the standard deviation)\n",
    "outlier_mask_ground = (data_ground - mean_ground).abs() > 3 * std_ground\n",
    "\n",
    "# Filter the DataFrame to keep only rows without outliers in the 'ground_21Mar1200_median' column\n",
    "ground_outlier_rooms = df_clean6_info_room[outlier_mask_ground]\n",
    "\n",
    "# Get the apartment_id values from the filtered data and store them in a list\n",
    "apartment_ids = ground_outlier_rooms['apartment_id'].tolist()\n",
    "print(len(set(apartment_ids)))\n",
    "\n",
    "# Exclude the rows with the filtered apartment_ids from the original DataFrame\n",
    "df_clean7_info_room_ground = df_clean6_info_room[~df_clean6_info_room['apartment_id'].isin(apartment_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_room_complete4 is your DataFrame\n",
    "data_daylight_Mar = df_clean6_info_room['daylight_21Mar1200_median']\n",
    "\n",
    "# Calculate the mean and standard deviation for the 'daylight_21Mar1200_median' column\n",
    "mean_daylight_Mar = data_daylight_Mar.mean()\n",
    "std_daylight_Mar = data_daylight_Mar.std()\n",
    "\n",
    "# Create a Boolean mask for outliers in the 'daylight_21Mar1200_median' column a threshold for outliers (e.g., 3 times the standard deviation)\n",
    "outlier_mask_daylight_Mar = (data_daylight_Mar - mean_daylight_Mar).abs() > 3 * std_daylight_Mar\n",
    "\n",
    "# Filter the DataFrame to keep only rows without outliers in the 'daylight_21Mar1200_median' column\n",
    "daylight_Mar_outlier_rooms = df_clean6_info_room[outlier_mask_daylight_Mar]\n",
    "\n",
    "# Get the apartment_id values from the filtered data and store them in a list\n",
    "apartment_ids = daylight_Mar_outlier_rooms['apartment_id'].tolist()\n",
    "print(len(set(apartment_ids)))\n",
    "\n",
    "# Exclude the rows with the filtered apartment_ids from the original DataFrame\n",
    "df_clean7_info_room_daylight_Mar = df_clean6_info_room[~df_clean6_info_room['apartment_id'].isin(apartment_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_room_complete4 is your DataFrame\n",
    "data_daylight_Jun = df_clean6_info_room['daylight_21Jun1200_median']\n",
    "\n",
    "# Calculate the mean and standard deviation for the 'daylight_21Jun1200_median' column\n",
    "mean_daylight_Jun = data_daylight_Jun.mean()\n",
    "std_daylight_Jun = data_daylight_Jun.std()\n",
    "\n",
    "# Create a Boolean mask for outliers in the 'daylight_21Jun1200_median' column a threshold for outliers (e.g., 3 times the standard deviation)\n",
    "outlier_mask_daylight_Jun = (data_daylight_Jun - mean_daylight_Jun).abs() > 3 * std_daylight_Jun\n",
    "\n",
    "# Filter the DataFrame to keep only rows without outliers in the 'daylight_21Jun1200_median' column\n",
    "daylight_Jun_outlier_rooms = df_clean6_info_room[outlier_mask_daylight_Jun]\n",
    "\n",
    "# Get the apartment_id values from the filtered data and store them in a list\n",
    "apartment_ids = daylight_Jun_outlier_rooms['apartment_id'].tolist()\n",
    "print(len(set(apartment_ids)))\n",
    "\n",
    "# Exclude the rows with the filtered apartment_ids from the original DataFrame\n",
    "df_clean7_info_room_daylight_Jun = df_clean6_info_room[~df_clean6_info_room['apartment_id'].isin(apartment_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_room_complete4 is your DataFrame\n",
    "data_daylight_Dec = df_clean6_info_room['daylight_21Dec1200_median']\n",
    "\n",
    "# Calculate the mean and standard deviation for the 'daylight_21Dec1200_median' column\n",
    "mean_daylight_Dec = data_daylight_Dec.mean()\n",
    "std_daylight_Dec = data_daylight_Dec.std()\n",
    "\n",
    "# Create a Boolean mask for outliers in the 'daylight_21Dec1200_median' column a threshold for outliers (e.g., 3 times the standard deviation)\n",
    "outlier_mask_daylight_Dec = (data_daylight_Dec - mean_daylight_Dec).abs() > 3 * std_daylight_Dec\n",
    "\n",
    "# Filter the DataFrame to keep only rows without outliers in the 'daylight_21Dec1200_median' column\n",
    "daylight_Dec_outlier_rooms = df_clean6_info_room[outlier_mask_daylight_Dec]\n",
    "\n",
    "# Get the apartment_id values from the filtered data and store them in a list\n",
    "apartment_ids = daylight_Dec_outlier_rooms['apartment_id'].tolist()\n",
    "print(len(set(apartment_ids)))\n",
    "\n",
    "# Exclude the rows with the filtered apartment_ids from the original DataFrame\n",
    "df_clean7_info_room_daylight_Dec = df_clean6_info_room[~df_clean6_info_room['apartment_id'].isin(apartment_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the apartment_id values from the filtered data and store them in sets\n",
    "apartment_ids_clean7_sky = set(df_clean7_info_room_sky['apartment_id'])\n",
    "apartment_ids_clean7_ground = set(df_clean7_info_room_ground['apartment_id'])\n",
    "apartment_ids_clean7_daylight_Mar = set(df_clean7_info_room_daylight_Mar['apartment_id'])\n",
    "apartment_ids_clean7_daylight_Jun = set(df_clean7_info_room_daylight_Jun['apartment_id'])\n",
    "apartment_ids_clean7_daylight_Dec = set(df_clean7_info_room_daylight_Dec['apartment_id'])\n",
    "\n",
    "# Find the common apartment_ids using set intersection\n",
    "common_apartment_ids = (\n",
    "    apartment_ids_clean7_sky &\n",
    "    apartment_ids_clean7_ground &\n",
    "    apartment_ids_clean7_daylight_Mar &\n",
    "    apartment_ids_clean7_daylight_Jun &\n",
    "    apartment_ids_clean7_daylight_Dec\n",
    ")\n",
    "\n",
    "# Exclude the rows with the filtered apartment_ids from the original DataFrame\n",
    "df_clean7_info_room_temp = df_clean6_info_room[df_clean6_info_room['apartment_id'].isin(common_apartment_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes based on 'apartment_id' and 'area_id'\n",
    "df_merged_info_room_temp = df_clean7_info_room_temp.merge(df_clean4_geom[['apartment_id', 'area_id', 'building_id', 'floor_id', 'height']], on=['apartment_id', 'area_id'], how='left')\n",
    "\n",
    "# Update the 'building_id' column in df_clean_info_room7 with values from df_clean7_geom\n",
    "df_clean7_info_room_temp['height'] = df_merged_info_room_temp['height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove additional outliers of ground view, room area & room height\n",
    "# Filter rows where [view_ground_p80] is greater than 15\n",
    "filtered_df = df_clean7_info_room_temp[(df_clean7_info_room_temp['view_landscape_urban_p80'] > 12) | (df_clean7_info_room_temp['view_landscape_nature_p80'] > 12) | (df_clean7_info_room_temp['layout_area'] > 100) | (df_clean7_info_room_temp['height'] > 3.4)]\n",
    "\n",
    "# Get the [apartment_id] values from the filtered DataFrame\n",
    "apartment_ids = filtered_df['apartment_id'].tolist()\n",
    "\n",
    "# Exclude the rows with the filtered apartment_ids from the original DataFrame\n",
    "df_clean7_info_room = df_clean7_info_room_temp[~df_clean7_info_room_temp['apartment_id'].isin(apartment_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the number of apartments that are daylight and/or sky view outliers\n",
    "num_app_outliers = df_clean6_info_room['apartment_id'].nunique() - df_clean7_info_room['apartment_id'].nunique()\n",
    "num_room_outliers = df_clean6_info_room['area_id'].nunique() - df_clean7_info_room['area_id'].nunique()\n",
    "print(f'Daylight and sky view outliers: {num_app_outliers}, Rooms: {num_room_outliers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean6_info_room['data_ground_current'] = MinMaxScaler().fit_transform(df_clean6_info_room['view_ground_p80'].values.reshape(-1, 1))\n",
    "df_clean6_info_room['data_sky_current'] = MinMaxScaler().fit_transform(df_clean6_info_room['view_sky_p80'].values.reshape(-1, 1))\n",
    "\n",
    "df_clean6_info_room['data_dayMar_current'] = MinMaxScaler().fit_transform(df_clean6_info_room['daylight_21Mar1200_median'].values.reshape(-1, 1))\n",
    "df_clean6_info_room['data_dayJun_current'] = MinMaxScaler().fit_transform(df_clean6_info_room['daylight_21Jun1200_median'].values.reshape(-1, 1))\n",
    "df_clean6_info_room['data_dayDec_current'] = MinMaxScaler().fit_transform(df_clean6_info_room['daylight_21Dec1200_median'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean7_info_room['data_ground_NOoutliers'] = MinMaxScaler().fit_transform(df_clean7_info_room['view_ground_p80'].values.reshape(-1, 1))\n",
    "df_clean7_info_room['data_sky_NOoutliers'] = MinMaxScaler().fit_transform(df_clean7_info_room['view_sky_p80'].values.reshape(-1, 1))\n",
    "\n",
    "df_clean7_info_room['data_dayMar_NOoutliers'] = MinMaxScaler().fit_transform(df_clean7_info_room['daylight_21Mar1200_median'].values.reshape(-1, 1))\n",
    "df_clean7_info_room['data_dayJun_NOoutliers'] = MinMaxScaler().fit_transform(df_clean7_info_room['daylight_21Jun1200_median'].values.reshape(-1, 1))\n",
    "df_clean7_info_room['data_dayDec_NOoutliers'] = MinMaxScaler().fit_transform(df_clean7_info_room['daylight_21Dec1200_median'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_path = f'C:/Users/Name/OneDrive - Delft University of Technology/Building Technology-Thesis/10_Images_04Dataset_data/Boxplots_v4/'\n",
    "scatterplot_path = f'C:/Users/Name/OneDrive - Delft University of Technology/Building Technology-Thesis/10_Images_04Dataset_data/Scatterplots_v4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns you want to plot - current state\n",
    "data_ground_current = df_clean6_info_room['data_ground_current']\n",
    "data_sky_current = df_clean6_info_room['data_sky_current']\n",
    "\n",
    "data_dayMar_current = df_clean6_info_room['data_dayMar_current']\n",
    "data_dayJun_current = df_clean6_info_room['data_dayJun_current']\n",
    "data_dayDec_current = df_clean6_info_room['data_dayDec_current']\n",
    "\n",
    "# state without outliers\n",
    "data_ground_NOoutliers = df_clean7_info_room['data_ground_NOoutliers']\n",
    "data_sky_NOoutliers = df_clean7_info_room['data_sky_NOoutliers']\n",
    "\n",
    "data_dayMar_NOoutliers = df_clean7_info_room['data_dayMar_NOoutliers']\n",
    "data_dayJun_NOoutliers = df_clean7_info_room['data_dayJun_NOoutliers']\n",
    "data_dayDec_NOoutliers = df_clean7_info_room['data_dayDec_NOoutliers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns you want to plot\n",
    "data = [data_ground_NOoutliers, data_sky_NOoutliers, data_dayDec_NOoutliers, data_dayJun_NOoutliers, data_dayMar_NOoutliers, data_ground_current, data_sky_current, data_dayDec_current, data_dayJun_current, data_dayMar_current]\n",
    "\n",
    "# Create groups by view type\n",
    "box_labels = ['without outliers', 'with outliers']\n",
    "names_colors = ['daylight 21 Mar median', 'daylight 21 Jun median', 'daylight 21 Dec median', 'sky view p80', 'ground view p80']\n",
    "\n",
    "# Define colors for each view\n",
    "colors = ['#084C61', '#65949F', '#903C59', '#D7A6B3', '#B0647E', '#084C61', '#65949F', '#903C59', '#D7A6B3', '#B0647E']\n",
    "colors1 = ['#B0647E', '#D7A6B3', '#903C59', '#65949F', '#084C61']\n",
    "\n",
    "# Set x-positions for boxes\n",
    "position = [0.2, 0.4, 0.6, 0.8, 1, 1.4, 1.6, 1.8, 2.0, 2.2]  # Adjust the positions as needed\n",
    "position1 = [0.6, 1.8]  # Adjust the positions as needed\n",
    "\n",
    "# Create a figure and axis objects\n",
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "# Plot\n",
    "bp = plt.boxplot(data, widths=0.15, patch_artist=True, vert=False, positions=position)\n",
    "\n",
    "# Set the colors for the faces of the boxes\n",
    "for i, box in enumerate(bp['boxes']):\n",
    "    box.set(facecolor=colors[i])\n",
    "\n",
    "# Make the median lines more visible\n",
    "plt.setp(bp['medians'], color='black')\n",
    "\n",
    "# Customize outliers, set smaller and transparent outliers\n",
    "for flier in bp['fliers']:\n",
    "    flier.set(marker='o', markerfacecolor='#808081', markersize=3, alpha=0.2)  \n",
    "\n",
    "# Set the labels and title\n",
    "ax.set_title('Normalised label values')\n",
    "\n",
    "# Set tick locations and labels\n",
    "ax.set_yticks(position1)\n",
    "ax.set_yticklabels(box_labels, rotation=90, ha='center', va='center')\n",
    "\n",
    "# Set x-axis limit\n",
    "ax.set_xlim([-0.01, 1.01])  # Adjust the x-axis limit as needed\n",
    "ax.set_ylim([0, 2.4])  # Adjust the x-axis limit as needed\n",
    "\n",
    "# Create custom legend at the top right corner\n",
    "legend_handles = []\n",
    "for i, name in enumerate(names_colors):\n",
    "    legend_handles.append(mpatches.Patch(color=colors1[i], label=name))\n",
    "\n",
    "# Adjust the legend position using bbox_to_anchor\n",
    "plt.legend(handles=legend_handles, title='ML label', loc='upper right')\n",
    "\n",
    "# Safe & show the plot\n",
    "plt.savefig(f'{boxplot_path}boxplot_removing_outliers.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "x_data1 = df_clean6_info_room['daylight_21Mar1200_median']\n",
    "x_data2 = df_clean7_info_room['daylight_21Mar1200_median']\n",
    "y_data1 = df_clean6_info_room['view_sky_p80']\n",
    "y_data2 = df_clean7_info_room['view_sky_p80']\n",
    "print('plot1: daylight max:', x_data1.max(), ' sky view max:', y_data1.max())\n",
    "print('plot2: daylight max:', x_data2.max(), ' sky view max:', y_data2.max())\n",
    "\n",
    "# Titles and labels\n",
    "title1 = 'Sky view vs. daylight with outliers'\n",
    "title2 = 'Sky view vs. daylight without outliers'\n",
    "x_name1 = 'Daylight at 21st March at 12:00 median [lx]'\n",
    "x_name2 = 'Daylight at 21st March at 12:00 median [lx]'\n",
    "y_name1 = 'View to sky p80 [%]'\n",
    "y_name2 = 'View to sky p80 [%]'\n",
    "\n",
    "# Create a figure with two subplots in a single row\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Scatter plots\n",
    "axes[0].scatter(x_data1, y_data1, s=20, color='#084C61', alpha=0.1)\n",
    "axes[1].scatter(x_data2, y_data2, s=20, color='#084C61', alpha=0.1)\n",
    "\n",
    "# Fit and plot linear regression lines\n",
    "for ax, x_data, y_data, x_name, y_name, title in zip(axes, [x_data1, x_data2], [y_data1, y_data2], [x_name1, x_name2], [y_name1, y_name2], [title1, title2]):\n",
    "    z = np.polyfit(x_data, y_data, 1)\n",
    "    p = np.poly1d(z)\n",
    "    \n",
    "    # Customize x and y-axis ranges for each subplot\n",
    "    if title == 'Sky view vs. daylight with outliers':  # Replace 'x_name1' with the appropriate condition\n",
    "        x_range = (0, 9000)  # Adjust the x-axis range as needed\n",
    "        y_range = (0, 16)    # Adjust the y-axis range as needed\n",
    "    else:\n",
    "        x_range = (0, 1700)\n",
    "        y_range = (0, 5.5)\n",
    "    \n",
    "    ax.plot(x_range, p(x_range), color='#F7D08A', label='Linear regression')\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x_name)\n",
    "    ax.set_ylabel(y_name)\n",
    "    \n",
    "    ax.set_xlim(x_range)  # Set the x-axis range\n",
    "    ax.set_ylim(y_range)  # Set the y-axis range\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save and show the plot\n",
    "plt.savefig(f'{scatterplot_path}scatterplot__removing_outliers.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the final apartment_ids\n",
    "apartment_ids = df_clean7_info_room['apartment_id'].tolist()\n",
    "\n",
    "# Only include the final apartments for the dataframes geometry, simulations, and fullt df_info\n",
    "df_clean7_geom = df_clean4_geom[df_clean4_geom['apartment_id'].isin(apartment_ids)]\n",
    "df_clean7_sim = df_clean4_sim[df_clean4_sim['apartment_id'].isin(apartment_ids)]\n",
    "df_clean7_sim = df_clean7_sim.dropna(subset=['sun_201803211200_median', 'view_sky_p80'], axis=0)\n",
    "\n",
    "# Assuming df_clean4_info and df_info_doors are your DataFrames\n",
    "df_clean4_info = pd.concat([df_clean4_info, df_info_doors], ignore_index=True)\n",
    "df_clean7_info = df_clean4_info[df_clean4_info['apartment_id'].isin(apartment_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes based on 'apartment_id' and 'area_id'\n",
    "df_merged_info = df_clean7_info.merge(df_clean7_geom[['apartment_id', 'area_id', 'building_id', 'floor_id', 'height']], on=['apartment_id', 'area_id'], how='left')\n",
    "\n",
    "# Update the 'building_id' column in df_clean_info7 with values from df_clean7_geom\n",
    "df_clean7_info['building_id'] = df_merged_info['building_id']\n",
    "df_clean7_info['floor_id'] = df_merged_info['floor_id']\n",
    "df_clean7_info['height'] = df_merged_info['height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the columns to place the new columns next to 'site_id'\n",
    "column_order = ['site_id', 'building_id', 'floor_id', 'apartment_id', 'area_id', 'entity_subtype', 'geometry', 'elevation', 'height', \n",
    "                'door_connection1', 'door_connection2', 'orientation', 'window_height', 'window_length', 'window_area']\n",
    "df_clean8_info = df_clean7_info[column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes based on 'apartment_id' and 'area_id'\n",
    "df_merged_info_room = df_clean7_info_room.merge(df_clean7_geom[['apartment_id', 'area_id', 'building_id', 'floor_id']], on=['apartment_id', 'area_id'], how='left')\n",
    "\n",
    "# Update the 'building_id' column in df_clean_info_room7 with values from df_clean7_geom\n",
    "df_clean7_info_room['building_id'] = df_merged_info_room['building_id']\n",
    "df_clean7_info_room['floor_id'] = df_merged_info_room['floor_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean7_info_room['view_landscape_p80'] = df_clean7_info_room['view_landscape_nature_p80'] + df_clean7_info_room['view_landscape_urban_p80']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the columns to place the new columns next to 'site_id'\n",
    "column_order = ['site_id', 'building_id', 'floor_id', 'apartment_id', 'area_id', 'entity_subtype', 'geometry', 'elevation', 'height', \n",
    "                'orientation', 'orientation_percentage', 'orientation_distribution', 'orientation_main',\n",
    "                'window_area', 'wall_area', 'window_wall_ratio', 'room_depth_ratio', 'layout_area', 'window_floor_ratio',\n",
    "                'view_site_mean', 'view_ground_mean', 'view_landscape_nature_mean', 'view_landscape_urban_mean', 'view_sky_mean', 'view_total_mean', \n",
    "                'view_site_median', 'view_ground_median', 'view_landscape_nature_median', 'view_landscape_urban_median', 'view_sky_median', \n",
    "                'view_site_p80', 'view_ground_p80', 'view_landscape_p80', 'view_landscape_nature_p80', 'view_landscape_urban_p80', 'view_sky_p80', \n",
    "                'daylight_21Mar1200_mean', 'daylight_21Jun1200_mean', 'daylight_21Dec1200_mean',\n",
    "                'daylight_21Mar1200_median', 'daylight_21Jun1200_median', 'daylight_21Dec1200_median',\n",
    "                'data_ground_NOoutliers', 'data_sky_NOoutliers', 'data_dayMar_NOoutliers', 'data_dayJun_NOoutliers', 'data_dayDec_NOoutliers']\n",
    "df_clean8_info_room = df_clean7_info_room[column_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'window_height' column to float data type, handling NaN values\n",
    "df_clean8_info['orientation'] = pd.to_numeric(df_clean8_info['orientation'], errors='coerce').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df_clean4_info is your DataFrame\n",
    "df_clean8_info_room = df_clean7_info_room.rename(columns={\n",
    "    'data_ground_NOoutliers': 'view_ground_p80_normalised',\n",
    "    'data_sky_NOoutliers': 'view_sky_p80_normalised',\n",
    "    'data_dayMar_NOoutliers': 'daylight_21Mar1200_median_normalised', \n",
    "    'data_dayJun_NOoutliers': 'daylight_21Jun1200_median_normalised',\n",
    "    'data_dayDec_NOoutliers': 'daylight_21Dec1200_median_normalised'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean8_info_room.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clean8_info_room['entity_subtype'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find performance levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns you want to keep in the new DataFrame\n",
    "columns_to_keep = ['apartment_id', 'site_id', 'area_id', 'entity_subtype', 'orientation_main','daylight_21Mar1200_median', 'daylight_21Jun1200_median', 'daylight_21Dec1200_median', 'view_ground_p80', 'view_landscape_p80', 'view_sky_p80']\n",
    "\n",
    "# Create the new DataFrame by selecting the specified columns\n",
    "df_clean8_performance = df_clean8_info_room[columns_to_keep].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daylight performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daylight levels for the rooms\n",
    "# Calculate the median along the specified columns and store it in a new column\n",
    "df_clean8_performance['daylight_median'] = df_clean8_performance[['daylight_21Mar1200_median', 'daylight_21Jun1200_median', 'daylight_21Dec1200_median']].median(axis=1)\n",
    "\n",
    "# Apply the function to the DataFrame and create a new column 'daylight_level'\n",
    "df_clean8_performance['daylight_level'] = df_clean8_performance.apply(\n",
    "    lambda row: find_daylight_level(row['daylight_median'], row['entity_subtype']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Apply the function to the DataFrame and create a new column 'daylight_level'\n",
    "df_clean8_performance['daylight_level_num'] = df_clean8_performance['daylight_level'].map(mapping_daylight_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the daylight labels for each apartment\n",
    "# Step 1: Group by 'apartment_id'\n",
    "grouped = df_clean8_performance.groupby('apartment_id')\n",
    "\n",
    "# Step 2: Calculate the median and min values for each apartment group\n",
    "daylight_level_median = grouped['daylight_level_num'].median()\n",
    "daylight_level_min = grouped['daylight_level_num'].min()\n",
    "\n",
    "# Create a dictionary to store 'daylight_label' for each apartment\n",
    "apartment_daylight_labels = {}\n",
    "\n",
    "# Step 3: Apply the function to each apartment group and update the dictionary\n",
    "for group_name, group_data in grouped:\n",
    "    daylight_label = find_daylight_label(group_data)\n",
    "    apartment_daylight_labels[group_name] = daylight_label\n",
    "\n",
    "# Step 4: Update 'daylight_label' in the original DataFrame for the rooms that belong to the apartment\n",
    "df_clean8_performance['daylight_label'] = df_clean8_performance['apartment_id'].map(apartment_daylight_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the DataFrame and create a new column 'daylight_level'\n",
    "df_clean8_performance['daylight_label_num'] = df_clean8_performance['daylight_label'].map(mapping_daylight_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean8_performance['landscape_visible'] = df_clean8_performance['view_landscape_p80'].apply(lambda x: 'yes' if x >= 0.477 else 'no')\n",
    "df_clean8_performance['visible_view_layers'] = df_clean8_performance.apply(count_visible_views, axis=1)\n",
    "\n",
    "# Apply the function to the DataFrame and create a new column 'view_level'\n",
    "df_clean8_performance['view_level'] = df_clean8_performance.apply(\n",
    "    lambda row: find_view_level(row['landscape_visible'], row['visible_view_layers']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Apply the function to the DataFrame and create a new column 'view_level'\n",
    "df_clean8_performance['view_level_num'] = df_clean8_performance['view_level'].map(mapping_view_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the view labels for each apartment\n",
    "# Step 1: Group by 'apartment_id'\n",
    "grouped = df_clean8_performance.groupby('apartment_id')\n",
    "\n",
    "# Step 2: Calculate the median and min values for each apartment group\n",
    "view_level_median = grouped['view_level_num'].median()\n",
    "view_level_min = grouped['view_level_num'].min()\n",
    "\n",
    "# Create a dictionary to store 'view_label' for each apartment\n",
    "apartment_view_labels = {}\n",
    "\n",
    "# Step 3: Apply the function to each apartment group and update the dictionary\n",
    "for group_name, group_data in grouped:\n",
    "    view_label = find_view_label(group_data)\n",
    "    apartment_view_labels[group_name] = view_label\n",
    "\n",
    "# Step 4: Update 'view_label' in the original DataFrame for the rooms that belong to the apartment\n",
    "df_clean8_performance['view_label'] = df_clean8_performance['apartment_id'].map(apartment_view_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the DataFrame and create a new column 'view_level'\n",
    "df_clean8_performance['view_label_num'] = df_clean8_performance['view_label'].map(mapping_view_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orientation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the apartment_id values from the filtered data and store them in a list\n",
    "apartment_ids = df_clean8_performance['apartment_id'].tolist()\n",
    "\n",
    "# Exclude the rows with the filtered apartment_ids from the original DataFrame\n",
    "df_clean8_outdoorspace = df_combinedinfo_outdoorspace[df_combinedinfo_outdoorspace['apartment_id'].isin(apartment_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the common columns that exist in both DataFrames\n",
    "common_columns = list(set(df_clean8_performance.columns) & set(df_clean8_outdoorspace.columns))\n",
    "\n",
    "# Select the common columns from df_clean8_outdoorspace\n",
    "selected_columns = df_clean8_outdoorspace[common_columns]\n",
    "\n",
    "# Concatenate the selected columns from df_clean8_outdoorspace under df_clean8_performance\n",
    "df_clean8_performance = pd.concat([df_clean8_performance, selected_columns], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the DataFrame and create a new column 'orientation_level'\n",
    "df_clean8_performance['orientation_level'] = df_clean8_performance.apply(\n",
    "    lambda row: find_orientation_level(row['entity_subtype'], row['orientation_main']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Apply the function to the DataFrame and create a new column 'orientation_level'\n",
    "df_clean8_performance['orientation_level_num'] = df_clean8_performance['orientation_level'].map(mapping_orientation_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the orientation labels for each apartment\n",
    "# Step 1: Group by 'apartment_id'\n",
    "grouped = df_clean8_performance.groupby('apartment_id')\n",
    "\n",
    "# Step 2: Calculate the median and min values for each apartment group\n",
    "orientation_level_median = grouped['orientation_level_num'].median()\n",
    "orientation_level_min = grouped['orientation_level_num'].min()\n",
    "\n",
    "# Create a dictionary to store 'orientation_label' for each apartment\n",
    "apartment_orientation_labels = {}\n",
    "\n",
    "# Step 3: Apply the function to each apartment group and update the dictionary\n",
    "for group_name, group_data in grouped:\n",
    "    orientation_label = find_orientation_label(group_data)\n",
    "    apartment_orientation_labels[group_name] = orientation_label\n",
    "\n",
    "# Step 4: Update 'orientation_label' in the original DataFrame for the rooms that belong to the apartment\n",
    "df_clean8_performance['orientation_label'] = df_clean8_performance['apartment_id'].map(apartment_orientation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the DataFrame and create a new column 'orientation_level'\n",
    "df_clean8_performance['orientation_label_num'] = df_clean8_performance['orientation_label'].map(mapping_orientation_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall apartment performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean8_performance['overall_label_num'] = df_clean8_performance.apply(\n",
    "    lambda row: find_overall_label_num(row['daylight_label_num'], row['view_label_num'], row['orientation_label_num']),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean8_performance['overall_label'] = df_clean8_performance.apply(\n",
    "    lambda row: find_overall_label(row['overall_label_num']),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have already computed the value counts\n",
    "filtered_df = df_clean8_performance.groupby('apartment_id').first().reset_index()\n",
    "value_counts = filtered_df['overall_label'].value_counts()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "value_counts.plot(kind='bar', color='skyblue')\n",
    "print(value_counts)\n",
    "# Customize the plot\n",
    "plt.title('Value Counts of overall Labels')\n",
    "plt.xlabel('overall Labels')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Show the plot\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for readability\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean8_performance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the index if necessary\n",
    "df_clean8_info_room.reset_index(drop=True, inplace=True)\n",
    "df_clean8_info.reset_index(drop=True, inplace=True)\n",
    "df_clean8_performance.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean8_info_room.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats of lost apartments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the number of duplicate rows in the dataframe\n",
    "num_rows_dub = df_geom.shape[0] - df_clean0A_geom.shape[0]\n",
    "print(\"Dubplicate rows:\", num_rows_dub, 'rows dropped')\n",
    "\n",
    "# Finding the number of feature rows in the dataframe\n",
    "num_rows_feat = df_clean0A_geom.shape[0] - df_clean0B_geom.shape[0]\n",
    "print(\"Feature rows:\", num_rows_feat, 'rows dropped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of dictionaries to hold the information\n",
    "data = []\n",
    "\n",
    "# Finding the number of original apartments\n",
    "num_app_OG = df_clean0B_geom['apartment_id'].nunique()\n",
    "num_room_OG = df_clean0B_geom['area_id'].nunique()\n",
    "data.append({'Description': 'Total number of apartments', 'Apartments': num_app_OG, 'Rooms': num_room_OG})\n",
    "\n",
    "# Finding the number of starting apartments\n",
    "num_app_start = df_combinedinfo1['apartment_id'].nunique()\n",
    "num_room_start = df_combinedinfo1['area_id'].nunique()\n",
    "data.append({'Description': 'Total number without redundant room types', 'Apartments': num_app_start, 'Rooms': num_room_start})\n",
    "\n",
    "# Finding the number of apartments with more than one story in the dataframe\n",
    "data.append({'Description': 'Multiple story apartments', 'Apartments': num_app_mult, 'Rooms': num_room_mult})\n",
    "\n",
    "# Finding the number of apartments without simulation data in the dataframe\n",
    "data.append({'Description': 'Apartments without simulation results', 'Apartments': num_app_sim, 'Rooms': num_room_sim})\n",
    "\n",
    "# Finding the number of apartments with shared windows\n",
    "data.append({'Description': 'Apartments shared windows', 'Apartments': num_app_shared, 'Rooms': num_room_shared})\n",
    "\n",
    "# Finding the number of other difficulties in the apartments\n",
    "data.append({'Description': 'Apartments other difficulties', 'Apartments': num_app_otherprob, 'Rooms': num_room_otherprob})\n",
    "\n",
    "#Drop rooms without daylight but with view to sky\n",
    "data.append({'Description': 'Rooms without daylight with sky view', 'Apartments': num_app_daysky, 'Rooms': num_room_daysky})\n",
    "\n",
    "# Finding the number of rooms without simulation values\n",
    "data.append({'Description': 'Rooms with nan simulation values', 'Apartments': num_app_nan, 'Rooms': num_room_nan})\n",
    "\n",
    "# Finding the number of apartments that are daylight and/or sky view outliers\n",
    "data.append({'Description': 'Daylight and sky view outliers', 'Apartments': num_app_outliers, 'Rooms': num_room_outliers})\n",
    "\n",
    "# Finding the number of final apartments\n",
    "num_app_final = df_clean8_info_room['apartment_id'].nunique()\n",
    "num_room_final = df_clean8_info_room['area_id'].nunique()\n",
    "data.append({'Description': 'Total number of final apartments', 'Apartments': num_app_final, 'Rooms': num_room_final})\n",
    "\n",
    "# Create a DataFrame from the list of dictionaries\n",
    "df_results = pd.DataFrame(data)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Safe dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "# Write the combined data to a new CSV file\n",
    "df_clean8_info.to_csv('C:/Users/Name/OneDrive - Delft University of Technology/Building Technology-Thesis/Swiss Dataset/SwissDataset_v3.0.0/SwissDataset_v3.0.0_clean_v3_info.csv', index=False)\n",
    "df_clean7_geom.to_csv('C:/Users/Name/OneDrive - Delft University of Technology/Building Technology-Thesis/Swiss Dataset/SwissDataset_v3.0.0/SwissDataset_v3.0.0_clean_v3_geom.csv', index=False)\n",
    "df_clean7_sim.to_csv('C:/Users/Name/OneDrive - Delft University of Technology/Building Technology-Thesis/Swiss Dataset/SwissDataset_v3.0.0/SwissDataset_v3.0.0_clean_v3_sim.csv', index=False)\n",
    "'''\n",
    "\n",
    "''' \n",
    "df_clean8_info_room.to_csv('C:/Users/Name/OneDrive - Delft University of Technology/Building Technology-Thesis/Swiss Dataset/SwissDataset_v3.0.0/SwissDataset_v3.0.0_clean_v3_room.csv', index=False)\n",
    "df_clean8_performance.to_csv('C:/Users/Name/OneDrive - Delft University of Technology/Building Technology-Thesis/Swiss Dataset/SwissDataset_v3.0.0/SwissDataset_v3.0.0_clean_v3_performance.csv', index=False)\n",
    "#'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
